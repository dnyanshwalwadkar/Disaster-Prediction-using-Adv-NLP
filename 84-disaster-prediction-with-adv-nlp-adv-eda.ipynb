{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"colab=False\nimport torch\nif colab:\n    from google.colab import drive\n    \n\n    drive.mount('/content/drive')\n    path_data_test=\"/content/drive/My Drive/IPP-M2-DS/Deep with python/Sentiment Analysis/Data/test.csv\"\n    path_data_train=\"/content/drive/My Drive/IPP-M2-DS/Deep with python/Sentiment Analysis/Data/train.csv\"\n    path_data_sub = \"/content/drive/My Drive/IPP-M2-DS/Deep with python/Sentiment Analysis/Data/sample_submission.csv\"\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"Device :\",device)\n    \nelse :\n    path_data_test=\"../input/nlp-getting-started/test.csv\"\n    path_data_train=\"../input/nlp-getting-started/train.csv\"\n    path_data_sub = \"../input/nlp-getting-started/sample_submission.csv\"\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"Device :\",device)","metadata":{"id":"x5TMjAQN8Jnp","outputId":"1fbcb2da-cfff-4be7-c09f-9992ab5a8e27","execution":{"iopub.status.busy":"2022-04-04T11:39:15.021989Z","iopub.execute_input":"2022-04-04T11:39:15.022305Z","iopub.status.idle":"2022-04-04T11:39:16.458855Z","shell.execute_reply.started":"2022-04-04T11:39:15.022211Z","shell.execute_reply":"2022-04-04T11:39:16.458025Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntweet.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:51:13.179403Z","iopub.execute_input":"2022-04-04T11:51:13.179992Z","iopub.status.idle":"2022-04-04T11:51:13.218408Z","shell.execute_reply.started":"2022-04-04T11:51:13.179954Z","shell.execute_reply":"2022-04-04T11:51:13.217591Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"concerning the packages you will have to install two packages on google colaboratory with the following commands:","metadata":{"id":"liTg271B68Zw"}},{"cell_type":"code","source":"#!pip install transformers\n#!pip install datasets\n\nimport os\nfrom tqdm import tqdm, trange\nimport random\nimport time\nimport pickle\nfrom sklearn.metrics import f1_score   \n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport pandas as pd\nimport math\nimport time\nimport pprint\npp = pprint.PrettyPrinter()\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.optim as optim\nfrom torchtext.legacy import data\nfrom torchtext.data.utils import get_tokenizer\n\nimport torch.nn as nn\n\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, get_scheduler\nfrom transformers import XLNetConfig,AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\nfrom datasets import Dataset, load_dataset, load_metric\nfrom sklearn.metrics import f1_score\nfrom keras.preprocessing.sequence import pad_sequences\n\n\nresults={}\nmodels_name = [\"BERTA\",\"BERT\",\"XLNET\",\"LSTM\"]\nfor model_name in models_name:\n  results[model_name] = {\"Train Loss\":[],\"Train Acc\":[],\"Train f1\":[],\n                         \"Val. Loss\": [],\"Val. Acc\":[],\"Val. f1\":[],\n                         \"test. Acc\":0,\"test. loss\":0,\"test. f1\":0,\n                         \"Training time\":0}\n\n","metadata":{"id":"D-cjAqIaUKvK","execution":{"iopub.status.busy":"2022-04-04T11:39:16.460464Z","iopub.execute_input":"2022-04-04T11:39:16.460861Z","iopub.status.idle":"2022-04-04T11:39:22.613315Z","shell.execute_reply.started":"2022-04-04T11:39:16.460822Z","shell.execute_reply":"2022-04-04T11:39:22.612564Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:51:31.672989Z","iopub.execute_input":"2022-04-04T11:51:31.673250Z","iopub.status.idle":"2022-04-04T11:51:31.827906Z","shell.execute_reply.started":"2022-04-04T11:51:31.673221Z","shell.execute_reply":"2022-04-04T11:51:31.827051Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:51:44.906945Z","iopub.execute_input":"2022-04-04T11:51:44.907514Z","iopub.status.idle":"2022-04-04T11:51:45.204730Z","shell.execute_reply.started":"2022-04-04T11:51:44.907473Z","shell.execute_reply":"2022-04-04T11:51:45.204057Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:51:59.636699Z","iopub.execute_input":"2022-04-04T11:51:59.637163Z","iopub.status.idle":"2022-04-04T11:52:00.126569Z","shell.execute_reply.started":"2022-04-04T11:51:59.637128Z","shell.execute_reply":"2022-04-04T11:52:00.125789Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:52:13.351798Z","iopub.execute_input":"2022-04-04T11:52:13.352076Z","iopub.status.idle":"2022-04-04T11:52:14.386953Z","shell.execute_reply.started":"2022-04-04T11:52:13.352046Z","shell.execute_reply":"2022-04-04T11:52:14.386294Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"tweet['target_mean'] = tweet.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=tweet.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=tweet.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntweet.drop(columns=['target_mean'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:54:01.009490Z","iopub.execute_input":"2022-04-04T11:54:01.009885Z","iopub.status.idle":"2022-04-04T11:54:04.262835Z","shell.execute_reply.started":"2022-04-04T11:54:01.009852Z","shell.execute_reply":"2022-04-04T11:54:04.262160Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from wordcloud import STOPWORDS\nimport string\ndf_train = tweet\ndf_test = test\n# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ndf_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ndf_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndf_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:57:46.094589Z","iopub.execute_input":"2022-04-04T11:57:46.094839Z","iopub.status.idle":"2022-04-04T11:57:46.643035Z","shell.execute_reply.started":"2022-04-04T11:57:46.094812Z","shell.execute_reply":"2022-04-04T11:57:46.642327Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\nDISASTER_TWEETS = df_train['target'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(df_train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')\n    sns.distplot(df_train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')\n\n    sns.distplot(df_train[feature], label='Training', ax=axes[i][1])\n    sns.distplot(df_test[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:58:09.226769Z","iopub.execute_input":"2022-04-04T11:58:09.227335Z","iopub.status.idle":"2022-04-04T11:58:16.395671Z","shell.execute_reply.started":"2022-04-04T11:58:09.227291Z","shell.execute_reply":"2022-04-04T11:58:16.394022Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"The results dictionary will allow you to store the results as the experiments progress.\n","metadata":{"id":"K9pCjHSsWsal"}},{"cell_type":"markdown","source":"<a id=\"Functions\"></a>\n# Functions\n\nDuring this notebook we will use 3 main functions. One for model training (```train```). One for validation (```evaluate```) and finally one to visualize the performance (```plot_results```) of our models. Note that the train and validation function takes the model you want to train as an argument. We do this because the 4 models do not have exactly the same types of arguments for training and they do not use the same dataloader.\n","metadata":{"id":"h-0r03LYz-6k"}},{"cell_type":"code","source":"def train(model,iterator,optimizer,criterion,train_LSTM=False,train_pretrain=False):\n  torch.cuda.empty_cache()\n  epoch_loss = 0.0\n  epoch_acc = 0.0\n  epoch_f1 = 0.0\n\n  model.train()\n  metric = load_metric(\"accuracy\")\n  metric2 = load_metric(\"f1\")\n  for batch in iterator:\n      optimizer.zero_grad()\n\n      if train_LSTM:\n\n        text,text_lengths = batch.text\n        predictions = model(text,text_lengths).squeeze()\n        loss = criterion(predictions,batch.target)\n\n        correct = (torch.round(predictions) == batch.target).float() \n        acc = correct.sum() / len(correct)\n\n        y_true= batch.target.cpu().detach().numpy()\n        predictions = torch.round(predictions).cpu().detach().numpy()\n        f1 = f1_score(y_true, predictions)\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        epoch_f1 += f1\n\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n      if train_pretrain:\n\n        b_input_ids = batch[\"input_ids\"].to(device)\n        b_input_mask = batch[\"attention_mask\"].to(device)\n        b_labels = batch[\"target\"].to(device)        \n        outputs = model(b_input_ids,token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n        loss = outputs.loss\n        predictions = outputs.logits\n        predictions = torch.argmax(predictions, dim=-1)\n        metric.add_batch(predictions=predictions, references=batch[\"target\"])\n        metric2.add_batch(predictions=predictions, references=batch[\"target\"])\n        epoch_loss += loss.cpu().detach().numpy()\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n  if  train_pretrain:\n     return epoch_loss / len(iterator), metric.compute()[\"accuracy\"], metric2.compute()[\"f1\"]\n\n  if not train_pretrain:\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)\n\ndef evaluate(model,iterator,criterion, train_LSTM=False,train_pretrain=False):\n\n    epoch_loss = 0.0\n    epoch_acc = 0.0\n    epoch_f1 = 0.0\n    \n    # deactivate the dropouts\n    model.eval()\n    metric = load_metric(\"accuracy\")\n    metric2 = load_metric(\"f1\")\n    # Sets require_grad flat False\n    with torch.no_grad():\n        for batch in iterator:\n            if train_LSTM:\n\n              text,text_lengths = batch.text\n              predictions = model(text,text_lengths).squeeze()\n              loss = criterion(predictions,batch.target)\n\n              correct = (torch.round(predictions) == batch.target).float() \n              acc = correct.sum() / len(correct)\n\n              y_true= batch.target.cpu().detach().numpy()\n              predictions = torch.round(predictions).cpu().detach().numpy()\n              f1 = f1_score(y_true, predictions)\n\n              epoch_loss += loss.item()\n              epoch_acc += acc.item()\n              epoch_f1 +=f1.item()\n\n\n            if train_pretrain:\n              \n              b_input_ids = batch[\"input_ids\"].to(device)\n              b_input_mask = batch[\"attention_mask\"].to(device)\n              b_labels = batch[\"target\"].to(device)        \n              outputs = model(b_input_ids,token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n              \n              loss = outputs.loss\n              predictions = outputs.logits\n              predictions = torch.argmax(predictions, dim=-1)\n              lr_scheduler.step()\n              metric.add_batch(predictions=predictions, references=batch[\"target\"])\n              metric2.add_batch(predictions=predictions, references=batch[\"target\"])\n              epoch_loss += loss.cpu().detach().numpy()\n\n    if  train_pretrain:\n      return epoch_loss / len(iterator), metric.compute()[\"accuracy\"], metric2.compute()[\"f1\"]\n    if not train_pretrain:\n      return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator)\n\n\ndef test(model,iterator,criterion, train_LSTM=False,train_pretrain=False):    \n    preds = []\n    idss = []\n    probas = []\n    with torch.no_grad():\n        for batch in iterator:\n            if train_LSTM:\n              text,text_lengths = batch.text\n              idss+= [int(i) for i in batch.id]\n              out = model(text,text_lengths)\n              preds+= torch.round(out).reshape(-1).int().tolist()\n              probas+= out.reshape(-1).tolist()\n             \n            if train_pretrain:\n              b_input_ids = batch[\"input_ids\"].to(device)\n              b_input_mask = batch[\"attention_mask\"].to(device)\n              outputs = model(b_input_ids,token_type_ids=None, \n                                  attention_mask=b_input_mask)\n              predictions = outputs.logits\n              idss+=[int(i) for i in batch[\"id\"]]\n              preds += torch.argmax(predictions, dim=-1).cpu().reshape(-1).int().tolist()\n                \n              predictions=predictions-predictions.min()\n              predictions = predictions[:,1]/predictions.sum(axis=1)\n              probas+= predictions.cpu().reshape(-1).tolist()\n            \n            \n        \n        mapp=dict(zip(idss,preds))\n        mapp2=dict(zip(idss,probas))\n        pred=pd.read_csv(path_data_sub)\n        pred[\"target\"]=pred[\"id\"].map(mapp)\n        pred2 = pred.copy()\n        pred2[\"probas\"]=pred2[\"id\"].map(mapp2)\n        return pred,pred2","metadata":{"id":"ejHgxJL1fqtK","execution":{"iopub.status.busy":"2022-04-04T11:39:22.615579Z","iopub.execute_input":"2022-04-04T11:39:22.615785Z","iopub.status.idle":"2022-04-04T11:39:22.648063Z","shell.execute_reply.started":"2022-04-04T11:39:22.615760Z","shell.execute_reply":"2022-04-04T11:39:22.647294Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def plot_results(model_name):\n  plt.style.use(\"seaborn\")\n  fig, (ax1,ax2,ax3) = plt.subplots(3,sharex=True)\n  ax1.set_title(model_name,fontsize=18,fontstyle='italic')\n\n  ax1.plot(results[model_name][\"Train Acc\"],label=\"Train Acc\",linewidth=3)\n  ax1.plot(results[model_name][\"Val. Acc\"],label=\"Val. Acc\",linewidth=3)\n  ax1.legend(loc='upper left', shadow=True)\n  ax1.set_xlabel('EPOCHS')\n  ax1.set_ylabel('Accuracy')\n\n  ax3.plot(results[model_name][\"Train Loss\"],label=\"Train Loss\",linewidth=3)\n  ax3.plot(results[model_name][\"Val. Loss\"],label=\"Val. Loss\",linewidth=3)\n  ax3.legend(loc='upper left', shadow=True)\n  ax3.set_xlabel('EPOCHS')\n  ax3.set_ylabel('Loss')\n\n  ax2.plot(results[model_name][\"Train f1\"],label=\"Train f1\",linewidth=3)\n  ax2.plot(results[model_name][\"Val. f1\"],label=\"Val. f1\",linewidth=3)\n  ax2.legend(loc='upper left', shadow=True)\n  ax2.set_xlabel('EPOCHS')\n  ax2.set_ylabel('f1')\n\n  fig.set_figheight(12.3)\n  fig.set_figwidth(10)\n","metadata":{"id":"6s4TC_6baDNV","execution":{"iopub.status.busy":"2022-04-04T11:39:22.651548Z","iopub.execute_input":"2022-04-04T11:39:22.652160Z","iopub.status.idle":"2022-04-04T11:39:22.662702Z","shell.execute_reply.started":"2022-04-04T11:39:22.652118Z","shell.execute_reply":"2022-04-04T11:39:22.661940Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Presentation of data.\n","metadata":{"id":"LWpUCDAkuvtE"}},{"cell_type":"code","source":"df = pd.read_csv(path_data_train)\ndf.head()","metadata":{"id":"SB7QTz1Ku3NB","outputId":"eecc8945-0598-4657-efaf-64d387f22104","execution":{"iopub.status.busy":"2022-04-04T11:39:22.664143Z","iopub.execute_input":"2022-04-04T11:39:22.664457Z","iopub.status.idle":"2022-04-04T11:39:22.739014Z","shell.execute_reply.started":"2022-04-04T11:39:22.664425Z","shell.execute_reply":"2022-04-04T11:39:22.737723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pp.pprint(df.loc[0][\"text\"])\nprint(\"target :\",df.loc[0][\"target\"])","metadata":{"id":"dwhAvrOuGb8G","outputId":"f2e2facf-3013-40c9-fbed-25afa797df8e","execution":{"iopub.status.busy":"2022-04-04T11:39:22.740960Z","iopub.execute_input":"2022-04-04T11:39:22.742002Z","iopub.status.idle":"2022-04-04T11:39:22.756399Z","shell.execute_reply.started":"2022-04-04T11:39:22.741931Z","shell.execute_reply":"2022-04-04T11:39:22.754628Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We have the same number of positive and negative reviews, a balanced dataset. We can use the accuracy to compare our results.\n","metadata":{"id":"u5xPGE3YGOF1"}},{"cell_type":"code","source":"X = df.text\ny = df.target\n\nfig = plt.figure(figsize=(5,5))\ncolors=[\"#1F77B4\",'#A87AD4']\npos=y[y == 1]\nneg=y[y == 0]\nck=[pos.count(),neg.count()]\nlegpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n                 autopct ='%1.1f%%', \n                 shadow = True,\n                 colors = colors,\n                 startangle = 45,\n                 explode=(0, 0.1))\nplt.title(\"Propotion of negative and positive reviews\")\nplt.show()","metadata":{"id":"gwg6elctHp65","outputId":"28cacbd1-40f3-49f9-866d-79f7a6058522","execution":{"iopub.status.busy":"2022-04-04T11:39:22.757944Z","iopub.execute_input":"2022-04-04T11:39:22.760406Z","iopub.status.idle":"2022-04-04T11:39:23.217393Z","shell.execute_reply.started":"2022-04-04T11:39:22.760367Z","shell.execute_reply":"2022-04-04T11:39:23.216652Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sns.displot(df[\"text\"].apply(lambda x : len(x.split(' '))).tolist())","metadata":{"id":"xILjdnNOgBlN","outputId":"7127150d-cc63-4250-e1d2-f04d52a0dba8","execution":{"iopub.status.busy":"2022-04-04T11:39:23.218522Z","iopub.execute_input":"2022-04-04T11:39:23.218923Z","iopub.status.idle":"2022-04-04T11:39:23.834409Z","shell.execute_reply.started":"2022-04-04T11:39:23.218885Z","shell.execute_reply":"2022-04-04T11:39:23.833769Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True)\nID = data.RawField()\nfields = [(\"id\",ID),(None,None),(None,None),('text',TEXT),(\"target\",LABEL)]\ntraining_data = data.TabularDataset(path=path_data_train,\n                                    format=\"csv\",\n                                    fields=fields,\n                                    skip_header=True\n                                   )\ntrain_data,valid_data = training_data.split(split_ratio=0.8)\ntest_data = data.TabularDataset(path=path_data_test,\n                                    format=\"csv\",\n                                    fields=[(\"id\",ID),(None,None),(None,None),('text',TEXT)],\n                                    skip_header=True\n                                   )","metadata":{"id":"kOsEYCDwylo2","execution":{"iopub.status.busy":"2022-04-04T11:39:23.837912Z","iopub.execute_input":"2022-04-04T11:39:23.839428Z","iopub.status.idle":"2022-04-04T11:39:31.500104Z","shell.execute_reply.started":"2022-04-04T11:39:23.839389Z","shell.execute_reply":"2022-04-04T11:39:31.499395Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"TEXT.build_vocab(train_data,min_freq=2)\nLABEL.build_vocab(train_data)\nBATCH_SIZE = 32\ntrain_iterator = data.BucketIterator(\n        dataset=train_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\n\ntest_iterator = data.BucketIterator(\n        dataset=test_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\n\nvalidation_iterator = data.BucketIterator(\n        dataset=valid_data,\n        batch_size=BATCH_SIZE,\n        repeat=False,\n        sort=False,\n        device = device)\nprint(\"Train size = {}, Test size = {}, valid size = {}\".format(len(train_data),len(test_data),len(valid_data)))","metadata":{"id":"YL0cw2kGgjsd","outputId":"56eae687-88b3-4c2e-8f91-c6cdab35ee90","execution":{"iopub.status.busy":"2022-04-04T11:39:31.503422Z","iopub.execute_input":"2022-04-04T11:39:31.503673Z","iopub.status.idle":"2022-04-04T11:39:31.579775Z","shell.execute_reply.started":"2022-04-04T11:39:31.503638Z","shell.execute_reply":"2022-04-04T11:39:31.578992Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# LSTM CLASSIFIER","metadata":{"id":"IN8snBXM47NU"}},{"cell_type":"markdown","source":"We will start from a randomly initialized embedding that we will train. We will also use a bidirectional LSTM. The second layer use the pack_padded_sequence function. without going into too much detail, the idea of ​​this function is to save calculation time but will not influence the result. As you can see this model uses an LSTM layer.\n\n\n With pytorch the lstm layer returns a lot of information to us, it is not always easy to see clearly which one to use. As a reminder, one of the problems with RNNs comes from gradient backpropagation. When the chain is too large, the gradient tends either to explode or to disappear, which makes learning long-term relationships very complicated for this type of model. Hence the interest of using an LSTM. The LSTM will solve this problem by having a long-term memory. Thus LSTM receives three sources of information at the input of each cell, the data, the short-term memory which is recorded in the hidden states and the long-term memory which is recorded in the cell states. The LSTM cell will also return three items.\n\n* output : which is composed of all the hidden states in the last layer.\n* hidden : hidden state which contains information about the previous time step.\n* Cell : cell which contains information about all previous information in the sentence.\n\n\n\nWe will also use a bidirectional LSTM. The idea is that we will pass the data in one direction but also in the other. Thus we will use the information contained in the past and the future to predict the words. By having a bidirectional LSTM we will therefore have two hidden states. One for the forward part and the other for the backward part of the LSTM (nothing to do with the gradient here). We will retrieve the last hidden states from the forward and backward part that we will concatenate before using it as features in a fully connected layer.\n\n\n\n\n\n\n\n","metadata":{"id":"np2JOTzatnbp"}},{"cell_type":"code","source":"\nclass classifier(nn.Module):\n    \n    #define all the layers used in model\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout):\n        super().__init__()          \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout,\n                           batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.act = nn.Sigmoid()\n        \n    def forward(self, text, text_lengths):\n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True,enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n        dense_outputs=self.fc(hidden)\n        outputs=self.act(dense_outputs)\n        return outputs","metadata":{"id":"PPhgeNUyuhFd","execution":{"iopub.status.busy":"2022-04-04T11:39:31.580865Z","iopub.execute_input":"2022-04-04T11:39:31.581097Z","iopub.status.idle":"2022-04-04T11:39:31.589564Z","shell.execute_reply.started":"2022-04-04T11:39:31.581065Z","shell.execute_reply":"2022-04-04T11:39:31.588769Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = classifier (vocab_size=len(TEXT.vocab),\n                    embedding_dim=100,\n                    hidden_dim=63,\n                    output_dim=1,\n                    n_layers=2,\n                    bidirectional=True,\n                    dropout=0.2).to(device)\ncriterion = nn.BCELoss().to(device)\noptimizer = optim.Adam(model.parameters(),lr=1e-4)","metadata":{"id":"8hW4F-_vxB99","execution":{"iopub.status.busy":"2022-04-04T11:39:31.591126Z","iopub.execute_input":"2022-04-04T11:39:31.591376Z","iopub.status.idle":"2022-04-04T11:39:39.431520Z","shell.execute_reply.started":"2022-04-04T11:39:31.591344Z","shell.execute_reply":"2022-04-04T11:39:39.430767Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"start =time.time()\nEPOCH_NUMBER=12\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc,train_f1 = train(model,train_iterator,optimizer,criterion,train_LSTM=True)\n    valid_loss,valid_acc,valid_f1  = evaluate(model,validation_iterator,criterion,train_LSTM=True)\n  \n    results[\"LSTM\"][\"Train Loss\"].append(train_loss)\n    results[\"LSTM\"][\"Train Acc\"].append(train_acc*100)\n    results[\"LSTM\"][\"Train f1\"].append(train_f1*100)\n\n    results[\"LSTM\"][\"Val. Loss\"].append(valid_loss)\n    results[\"LSTM\"][\"Val. Acc\"].append(valid_acc*100)\n    results[\"LSTM\"][\"Val. f1\"].append(valid_f1*100)\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train f1: {train_f1*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  val. f1: {valid_f1*100:.2f}%')\n    print()","metadata":{"id":"R2kFIijHxmNU","outputId":"3c13dade-9c71-44a5-c541-909b1e06dbb7","execution":{"iopub.status.busy":"2022-04-04T11:39:39.432950Z","iopub.execute_input":"2022-04-04T11:39:39.433193Z","iopub.status.idle":"2022-04-04T11:41:08.972681Z","shell.execute_reply.started":"2022-04-04T11:39:39.433160Z","shell.execute_reply":"2022-04-04T11:41:08.971943Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pred_LSTM, probas_LSTM = test(model,test_iterator,criterion,train_LSTM=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:41:08.974006Z","iopub.execute_input":"2022-04-04T11:41:08.974246Z","iopub.status.idle":"2022-04-04T11:41:09.381424Z","shell.execute_reply.started":"2022-04-04T11:41:08.974212Z","shell.execute_reply":"2022-04-04T11:41:09.380717Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plot_results(\"LSTM\")","metadata":{"id":"_6fCQy_1fr0i","outputId":"1ea4a8fe-6e23-4d68-b49e-7b5bcc7e2bff","execution":{"iopub.status.busy":"2022-04-04T11:41:09.382630Z","iopub.execute_input":"2022-04-04T11:41:09.382870Z","iopub.status.idle":"2022-04-04T11:41:09.907831Z","shell.execute_reply.started":"2022-04-04T11:41:09.382840Z","shell.execute_reply":"2022-04-04T11:41:09.907039Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning\n\nWe will now fine tune transformers models. We are going to look at two more complex models now and see if their use really makes sense in the case of a dataset like Dataset. During this part on fine tuning we will focus on two well-known models [BERT](https://arxiv.org/pdf/1810.04805.pdf) and [XLNET](https://arxiv.org/pdf/1906.08237.pdf). To fine tune these models we will use the very practical \"transformers\" library of [hugging face](https://huggingface.co/). \n\n\n","metadata":{"id":"G6zC4zDkvUqp"}},{"cell_type":"markdown","source":"I'll start by quickly explaining what a transformer is. Transformers were proposed in 2017 in this [paper](https://arxiv.org/pdf/1706.03762.pdf), and have since been state-of-the-art in NLP.  In addition, the great progress of these models is that it will take the entire sentence as input and no longer sequentially as before. This will greatly improve computation times. \n\n\n","metadata":{"id":"RUJVpV6qDGsz"}},{"cell_type":"code","source":"model_max_length=50\nbatch_size=32\nEPOCH_NUMBER=8","metadata":{"id":"hOmVR7yrve90","execution":{"iopub.status.busy":"2022-04-04T11:41:09.908810Z","iopub.execute_input":"2022-04-04T11:41:09.909051Z","iopub.status.idle":"2022-04-04T11:41:09.914783Z","shell.execute_reply.started":"2022-04-04T11:41:09.909017Z","shell.execute_reply":"2022-04-04T11:41:09.913969Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# XLNET\n\nIn the family of transformers there are a large number of different architectures. One of the most popular is XLNET. The idea of ​​this method is to calculate the probability of a word based on all the permutations of all the other words in the sentence. Here we don't use the right and left context as an LSTM but we use permutations of words as context to predict a word.\n\n\nAs I use the transformer library for this fine tuning part. I have to re-create my dataloaders. I will therefore use the same distribution train, test, valid.\n","metadata":{"id":"12v0tV2DlWkh"}},{"cell_type":"code","source":"raw_datasets = load_dataset('csv', data_files=path_data_train)\nraw_datasets = raw_datasets[\"train\"].train_test_split(0.2)\nraw_datasets_test = load_dataset('csv', data_files=path_data_test)\nraw_datasets_test","metadata":{"id":"p4A7ao-WPNOA","outputId":"ac6badbf-87a6-41d0-e85c-161ff0d9af3b","execution":{"iopub.status.busy":"2022-04-04T11:41:09.915926Z","iopub.execute_input":"2022-04-04T11:41:09.916962Z","iopub.status.idle":"2022-04-04T11:41:12.226077Z","shell.execute_reply.started":"2022-04-04T11:41:09.916925Z","shell.execute_reply":"2022-04-04T11:41:12.225409Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Also the tokenizer that must be used is the one associated with XLNET. Indeed, as we are going to use an already trained model, it must take as input the same type of tokens, but also the same coding of a word to a number. For this we use AutoTokenizer.from_pretrained which allows to load the tokenizer.\n","metadata":{"id":"DGMR0s5xIgJQ"}},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\n\ntokenizer  = AutoTokenizer.from_pretrained(\"xlnet-base-cased\",model_max_length=model_max_length)\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets_test = raw_datasets_test.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"id\",\"keyword\",\"location\",\"text\"])\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets_test = tokenized_datasets_test.remove_columns(['keyword', 'location','text'])\ntokenized_datasets_test.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"id":"5PuwNULMPM_E","outputId":"b112b42b-3c99-41c8-a0fc-0fbfdab1149b","execution":{"iopub.status.busy":"2022-04-04T11:41:12.227353Z","iopub.execute_input":"2022-04-04T11:41:12.228140Z","iopub.status.idle":"2022-04-04T11:41:27.330061Z","shell.execute_reply.started":"2022-04-04T11:41:12.228104Z","shell.execute_reply":"2022-04-04T11:41:27.329330Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"As we can see the tokenizer sends us several new elements\n* ```input_ids``` : As before, input_ids corresponds to the tokens that we replaced with a number to give it as input to our model.\n* ```token_type_ids``` : Token type ids has an operation directly linked to the training of the XLNET model. During its training we said that XLNET should predict the next words using the permutations of all the other words. That's true, but that's not the only problem this model has to solve. The second task concerns a pair of sentences that we will give him as input and of which he will have to say whether they are in the right order or not. This parameter is used to indicate for the value 0 it is the first sentence the value 1 is the second and the value 2 is the end of sentence token. As here we don't have this type of objective, we are not interested in this vector.\n* ```attention_mask``` : This vector allows to know if my sequence has been received a padding or not. Here as we took sequences of size 128 we have almost none of our sequences that had a padding. As our sequences almost never have any padding, we have a vector with only 1s, if we had the last 10 values ​​that had been padded, we would have had 10 0s at the end of this vector.\n","metadata":{"id":"rXKtNIDrLJSC"}},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"input_ids\"][0]","metadata":{"id":"vNQuTjVfNoOs","outputId":"763183fc-ea69-4447-9ea6-4ea74d72ed10","execution":{"iopub.status.busy":"2022-04-04T11:41:27.331391Z","iopub.execute_input":"2022-04-04T11:41:27.331642Z","iopub.status.idle":"2022-04-04T11:41:27.462800Z","shell.execute_reply.started":"2022-04-04T11:41:27.331608Z","shell.execute_reply":"2022-04-04T11:41:27.462130Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"token_type_ids\"][0]","metadata":{"id":"JBlICt4ZNnCm","outputId":"ea7412d8-c9b6-4e0b-f65b-bc011aa5c383","execution":{"iopub.status.busy":"2022-04-04T11:41:27.464066Z","iopub.execute_input":"2022-04-04T11:41:27.464464Z","iopub.status.idle":"2022-04-04T11:41:27.588411Z","shell.execute_reply.started":"2022-04-04T11:41:27.464428Z","shell.execute_reply":"2022-04-04T11:41:27.587762Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][\"attention_mask\"][0]","metadata":{"id":"1Haj-iz_Iklb","outputId":"88e4e563-80c2-495d-c5d2-18888ff4d586","execution":{"iopub.status.busy":"2022-04-04T11:41:27.589580Z","iopub.execute_input":"2022-04-04T11:41:27.589810Z","iopub.status.idle":"2022-04-04T11:41:27.712763Z","shell.execute_reply.started":"2022-04-04T11:41:27.589777Z","shell.execute_reply":"2022-04-04T11:41:27.712113Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets_test[\"train\"], batch_size=batch_size, collate_fn=data_collator\n)\n","metadata":{"id":"EB82Z-P1PM3O","execution":{"iopub.status.busy":"2022-04-04T11:41:27.714087Z","iopub.execute_input":"2022-04-04T11:41:27.714354Z","iopub.status.idle":"2022-04-04T11:41:27.719406Z","shell.execute_reply.started":"2022-04-04T11:41:27.714320Z","shell.execute_reply":"2022-04-04T11:41:27.718528Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = AutoModelForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=EPOCH_NUMBER * len(train_dataloader),)","metadata":{"id":"Z_kp05JKYNUv","outputId":"3a696b9b-ad6b-43c8-a2c0-603825bd7e3b","execution":{"iopub.status.busy":"2022-04-04T11:41:27.720868Z","iopub.execute_input":"2022-04-04T11:41:27.721119Z","iopub.status.idle":"2022-04-04T11:41:59.336568Z","shell.execute_reply.started":"2022-04-04T11:41:27.721085Z","shell.execute_reply":"2022-04-04T11:41:59.334989Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nEPOCH_NUMBER=2\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc,train_f1 = train(model,train_dataloader,optimizer,None,train_pretrain=True)\n    valid_loss,valid_acc,valid_f1 = evaluate(model,eval_dataloader,None,train_pretrain=True)\n    \n    results[\"XLNET\"][\"Train Loss\"].append(train_loss)\n    results[\"XLNET\"][\"Train Acc\"].append(train_acc*100)\n    results[\"XLNET\"][\"Train f1\"].append(train_f1*100)\n\n    results[\"XLNET\"][\"Val. Loss\"].append(valid_loss)\n    results[\"XLNET\"][\"Val. Acc\"].append(valid_acc*100)\n    results[\"XLNET\"][\"Val. f1\"].append(valid_f1*100)\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train f1: {train_f1*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  val. f1: {valid_f1*100:.2f}%')\n    print()","metadata":{"id":"yoJNDbJfWm8K","outputId":"a1362dd3-d7bf-465d-aa6a-cc610b788707","execution":{"iopub.status.busy":"2022-04-04T11:41:59.337739Z","iopub.execute_input":"2022-04-04T11:41:59.337983Z","iopub.status.idle":"2022-04-04T11:43:31.586698Z","shell.execute_reply.started":"2022-04-04T11:41:59.337948Z","shell.execute_reply":"2022-04-04T11:43:31.585964Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"preds = []\nidss = []\nprobas = []\nwith torch.no_grad():\n    for batch in test_dataloader:\n      b_input_ids = batch[\"input_ids\"].to(device)\n      b_input_mask = batch[\"attention_mask\"].to(device)\n      outputs = model(b_input_ids,token_type_ids=None, \n                          attention_mask=b_input_mask)\n      predictions = outputs.logits\n      idss+=[int(i) for i in batch[\"id\"]]\n      preds += torch.argmax(predictions, dim=-1).cpu().reshape(-1).int().tolist()\n      \n      predictions=predictions-predictions.min()\n      predictions = predictions[:,1]/predictions.sum(axis=1)\n    \n      probas+= predictions.cpu().reshape(-1).tolist()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:43:31.588120Z","iopub.execute_input":"2022-04-04T11:43:31.588382Z","iopub.status.idle":"2022-04-04T11:43:39.157087Z","shell.execute_reply.started":"2022-04-04T11:43:31.588347Z","shell.execute_reply":"2022-04-04T11:43:39.156393Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"pred_XLNET, probas_XLNET = test(model,test_dataloader,None,train_pretrain=True)","metadata":{"id":"_NcD0HVzXXUK","execution":{"iopub.status.busy":"2022-04-04T11:43:39.158526Z","iopub.execute_input":"2022-04-04T11:43:39.158886Z","iopub.status.idle":"2022-04-04T11:43:46.711007Z","shell.execute_reply.started":"2022-04-04T11:43:39.158847Z","shell.execute_reply":"2022-04-04T11:43:46.710317Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"plot_results('XLNET')","metadata":{"id":"tNHIMKKifyh_","outputId":"22b488b4-7de9-4bf0-cd7a-be2a97f6da09","execution":{"iopub.status.busy":"2022-04-04T11:43:46.712364Z","iopub.execute_input":"2022-04-04T11:43:46.712615Z","iopub.status.idle":"2022-04-04T11:43:47.249065Z","shell.execute_reply.started":"2022-04-04T11:43:46.712582Z","shell.execute_reply":"2022-04-04T11:43:47.247966Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# BERT\n\nBERT is probably the most popular transformer. It has long been the most efficient model. His strongest idea was to modify the way of training the model. Like XLNET, it is trained on two tasks. One is for sentence pairs like XLNET. Bert is trained to predict whether the second sentence is the sentence following the first in the original text. This type of training is called next sentence prediction NSP. But what made BERT known is above all the second task on which he trains. The idea is to mask a certain percentage of input tokens and train the model to predict these tokens using all unmasked tokens. in the figure below taken from the original paper. We can actually see these two spots on the figure on the left. On the figure on the right are examples of possible fine-tuning on three different datasets.\n\n\n\n\n\n\n<img src=\"https://miro.medium.com/max/1400/1*bYO5tEcRzdHtjHV_P6-4ig.png\" width=\"800\" class=\"center\"/>\n\n\n\n","metadata":{"id":"-VmOR0kABpvU"}},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\n\ntokenizer  = AutoTokenizer.from_pretrained(\"bert-base-cased\",model_max_length=model_max_length)\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets_test = raw_datasets_test.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"id\",\"keyword\",\"location\",\"text\"])\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets_test = tokenized_datasets_test.remove_columns(['keyword', 'location','text'])\ntokenized_datasets_test.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"id":"Kybir9NULGgp","outputId":"d20cd2f7-8a16-48ed-8050-b6e19b7b8126","execution":{"iopub.status.busy":"2022-04-04T11:43:47.259595Z","iopub.execute_input":"2022-04-04T11:43:47.260094Z","iopub.status.idle":"2022-04-04T11:44:02.787898Z","shell.execute_reply.started":"2022-04-04T11:43:47.260030Z","shell.execute_reply":"2022-04-04T11:44:02.787117Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets_test[\"train\"], batch_size=batch_size, collate_fn=data_collator\n)\n","metadata":{"id":"uRAnW9D_Bxb3","execution":{"iopub.status.busy":"2022-04-04T11:44:02.789265Z","iopub.execute_input":"2022-04-04T11:44:02.789536Z","iopub.status.idle":"2022-04-04T11:44:02.820012Z","shell.execute_reply.started":"2022-04-04T11:44:02.789501Z","shell.execute_reply":"2022-04-04T11:44:02.817554Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=EPOCH_NUMBER * len(train_dataloader),)","metadata":{"id":"1aRJd3iuB5ao","outputId":"7a448ade-3722-44e7-d848-f9ad868790df","execution":{"iopub.status.busy":"2022-04-04T11:44:02.821142Z","iopub.execute_input":"2022-04-04T11:44:02.821473Z","iopub.status.idle":"2022-04-04T11:44:33.271438Z","shell.execute_reply.started":"2022-04-04T11:44:02.821435Z","shell.execute_reply":"2022-04-04T11:44:33.270687Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc,train_f1 = train(model,train_dataloader,optimizer,None,train_pretrain=True)\n    valid_loss,valid_acc,valid_f1 = evaluate(model,eval_dataloader,None,train_pretrain=True)\n    \n    results[\"BERT\"][\"Train Loss\"].append(train_loss)\n    results[\"BERT\"][\"Train Acc\"].append(train_acc*100)\n    results[\"BERT\"][\"Train f1\"].append(train_f1*100)\n\n    results[\"BERT\"][\"Val. Loss\"].append(valid_loss)\n    results[\"BERT\"][\"Val. Acc\"].append(valid_acc*100)\n    results[\"BERT\"][\"Val. f1\"].append(valid_f1*100)\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train f1: {train_f1*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  val. f1: {valid_f1*100:.2f}%')\n    print()","metadata":{"id":"tqUoVSEZNXFC","outputId":"3e3118da-656b-4a82-ae14-dad276080eee","execution":{"iopub.status.busy":"2022-04-04T11:44:33.272904Z","iopub.execute_input":"2022-04-04T11:44:33.273400Z","iopub.status.idle":"2022-04-04T11:45:54.729477Z","shell.execute_reply.started":"2022-04-04T11:44:33.273359Z","shell.execute_reply":"2022-04-04T11:45:54.728736Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pred_BERT, probas_BERT = test(model,test_dataloader,None,train_pretrain=True)","metadata":{"id":"yV0vC0lkZVv8","execution":{"iopub.status.busy":"2022-04-04T11:45:54.730826Z","iopub.execute_input":"2022-04-04T11:45:54.731189Z","iopub.status.idle":"2022-04-04T11:46:01.233637Z","shell.execute_reply.started":"2022-04-04T11:45:54.731153Z","shell.execute_reply":"2022-04-04T11:46:01.232958Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"plot_results('BERT')","metadata":{"id":"4Hhi4feSjANt","outputId":"856d20f0-3c6b-40b8-e706-7a90feb80e1c","execution":{"iopub.status.busy":"2022-04-04T11:46:01.234879Z","iopub.execute_input":"2022-04-04T11:46:01.235123Z","iopub.status.idle":"2022-04-04T11:46:01.746083Z","shell.execute_reply.started":"2022-04-04T11:46:01.235090Z","shell.execute_reply":"2022-04-04T11:46:01.745425Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"As before, we have a model that was trained on an epochs top. We see that two epochs was enough. ","metadata":{"id":"ZTd3qFQeUxuC"}},{"cell_type":"markdown","source":"# ROBERTA","metadata":{}},{"cell_type":"code","source":"\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\n\ntokenizer  = AutoTokenizer.from_pretrained(\"roberta-base\",model_max_length=model_max_length)\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets_test = raw_datasets_test.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntokenized_datasets = tokenized_datasets.remove_columns([\"id\",\"keyword\",\"location\",\"text\"])\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets_test = tokenized_datasets_test.remove_columns(['keyword', 'location','text'])\ntokenized_datasets_test.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:46:01.747316Z","iopub.execute_input":"2022-04-04T11:46:01.747672Z","iopub.status.idle":"2022-04-04T11:46:19.689103Z","shell.execute_reply.started":"2022-04-04T11:46:01.747639Z","shell.execute_reply":"2022-04-04T11:46:19.688332Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n)\ntest_dataloader = DataLoader(\n    tokenized_datasets_test[\"train\"], batch_size=batch_size, collate_fn=data_collator\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:46:19.690542Z","iopub.execute_input":"2022-04-04T11:46:19.690794Z","iopub.status.idle":"2022-04-04T11:46:19.702429Z","shell.execute_reply.started":"2022-04-04T11:46:19.690760Z","shell.execute_reply":"2022-04-04T11:46:19.700893Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"linear\",optimizer=optimizer,num_warmup_steps=0,num_training_steps=EPOCH_NUMBER * len(train_dataloader),)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:46:19.703946Z","iopub.execute_input":"2022-04-04T11:46:19.704206Z","iopub.status.idle":"2022-04-04T11:46:52.830381Z","shell.execute_reply.started":"2022-04-04T11:46:19.704172Z","shell.execute_reply":"2022-04-04T11:46:52.828697Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nEPOCH_NUMBER=3\nfor epoch in range(1,EPOCH_NUMBER+1):\n    print(f\"\\t Epoch: {epoch}\")\n    train_loss,train_acc,train_f1 = train(model,train_dataloader,optimizer,None,train_pretrain=True)\n    valid_loss,valid_acc,valid_f1 = evaluate(model,eval_dataloader,None,train_pretrain=True)\n    \n    results[\"BERTA\"][\"Train Loss\"].append(train_loss)\n    results[\"BERTA\"][\"Train Acc\"].append(train_acc*100)\n    results[\"BERTA\"][\"Train f1\"].append(train_f1*100)\n\n    results[\"BERTA\"][\"Val. Loss\"].append(valid_loss)\n    results[\"BERTA\"][\"Val. Acc\"].append(valid_acc*100)\n    results[\"BERTA\"][\"Val. f1\"].append(valid_f1*100)\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train f1: {train_f1*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} |  val. f1: {valid_f1*100:.2f}%')\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:46:52.832315Z","iopub.execute_input":"2022-04-04T11:46:52.832572Z","iopub.status.idle":"2022-04-04T11:48:53.296084Z","shell.execute_reply.started":"2022-04-04T11:46:52.832527Z","shell.execute_reply":"2022-04-04T11:48:53.295398Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"pred_BERTA,probas_BERTA = test(model,test_dataloader,None,train_pretrain=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:48:53.297424Z","iopub.execute_input":"2022-04-04T11:48:53.297680Z","iopub.status.idle":"2022-04-04T11:48:59.387856Z","shell.execute_reply.started":"2022-04-04T11:48:53.297647Z","shell.execute_reply":"2022-04-04T11:48:59.387131Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Bagging and final Results\n\nIn order to improve overall performance when faced with a data challenge, it is good as a last step to combine these different models to make our submission. Indeed, the models will sometimes be good on different tweets, so taking their aggregation will allow us to get the best out of each one. Generally, the more different our models are, the more influence this last step will have on the final score. Here to aggregate the four models, I will use as a weight the results they had on the validation sample. I will then multiply these values ​​over the probabilities to get my final prediction.\n","metadata":{"id":"ul1JXVdeMkhx"}},{"cell_type":"code","source":"names=[\"BERT\",\"BERTA\",\"XLNET\"]\nweight=[]\nfor name in names:\n    print(name)\n    weight+=[results[name][\"Val. f1\"][-1]]\n   \nweight = np.array(weight)\nweight=weight/sum(weight)\nprint(weight)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:48:59.389176Z","iopub.execute_input":"2022-04-04T11:48:59.389439Z","iopub.status.idle":"2022-04-04T11:48:59.396133Z","shell.execute_reply.started":"2022-04-04T11:48:59.389404Z","shell.execute_reply":"2022-04-04T11:48:59.395325Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"pred_vec = probas_BERT[\"probas\"]*weight[0]+\\\nprobas_BERTA[\"probas\"]*weight[1]+\\\nprobas_XLNET[\"probas\"]*weight[2]\nprobas_vec=round(pred_vec)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:48:59.397949Z","iopub.execute_input":"2022-04-04T11:48:59.398507Z","iopub.status.idle":"2022-04-04T11:48:59.409098Z","shell.execute_reply.started":"2022-04-04T11:48:59.398469Z","shell.execute_reply":"2022-04-04T11:48:59.408310Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"pred=pd.read_csv(path_data_sub)\npred[\"target\"] = probas_vec.values\npred[\"target\"]=pred[\"target\"].astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:48:59.410454Z","iopub.execute_input":"2022-04-04T11:48:59.410699Z","iopub.status.idle":"2022-04-04T11:48:59.421689Z","shell.execute_reply.started":"2022-04-04T11:48:59.410667Z","shell.execute_reply":"2022-04-04T11:48:59.420853Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Summary\n* BERT : 0.83083\n* XLNET : 0.81949\n* GLOVE_Wiki300 : 0.78547\n* LSTM : 0.78761\n* BERTA : 0.83021\n* XLNET + BERT : 0.83481\n* XLNET + BERT + BERTA : 0.83665\n","metadata":{}},{"cell_type":"code","source":"#pred.to_csv('pred_bagg.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T11:50:33.407983Z","iopub.execute_input":"2022-04-04T11:50:33.408260Z","iopub.status.idle":"2022-04-04T11:50:33.420707Z","shell.execute_reply.started":"2022-04-04T11:50:33.408229Z","shell.execute_reply":"2022-04-04T11:50:33.420066Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}